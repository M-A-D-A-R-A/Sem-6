{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = (0,1,2,3)  # actions (0=up, 1=right,2=down,3=left)\n",
    "states = (0, 1, 2, 3)  # states (tiles)\n",
    "rewards = [-1, -1, -1, 10]  # Direct rewards per state\n",
    "gamma = 0.9  \n",
    "delta = 10**-2\n",
    "# Transition probabilities per state-action pair\n",
    "probs = [\n",
    "    [[0.9,0.1,0,0], [0.1,0.8,0.1,0], [0.1,0.1,0.8,0], [0.9,0,0.1,0]],\n",
    "    [[0.1,0.9,0,0], [0,0.9,0,0.1]  , [0.1,0.1,0,0.8], [0.8,0.1,0,0.1]],\n",
    "    [[0.8,0,0.1,0.1], [0.1,0,0.1,0.8] , [0,0,0.9,0.1], [0.1,0,0.9,0]],\n",
    "    [[0,0,0,0]    , [0,0,0,0]    , [0,0,0,0]    , [0,0,0,0]] # Term state (all probs 0)\n",
    "]\n",
    "\n",
    "\n",
    "max_policy_iter = 10000  # Maximum number of policy iterations\n",
    "max_value_iter = 10000  # Maximum number of value iterations\n",
    "pi = [0 for s in states]\n",
    "V = [0 for s in states]\n",
    "\n",
    "print(V,pi)\n",
    "for i in range(max_policy_iter):\n",
    "\n",
    "    optimal_policy_found = True\n",
    "\n",
    "    # Policy evaluation\n",
    "    # Compute value for each state under current policy\n",
    "    for j in range(max_value_iter):\n",
    "        max_diff = 0  # Initialize max difference\n",
    "        V_new = [0, 0, 0, 0]  # Initialize values\n",
    "        for s in states:\n",
    "\n",
    "            # Compute state value\n",
    "            val = rewards[s]  # Get direct reward\n",
    "            for s_next in states:\n",
    "                val += probs[s][s_next][pi[s]] * (\n",
    "                        gamma * V[s_next]\n",
    "                )  \n",
    "            if np.isnan(np.min(val)):\n",
    "                print(\"wtf\")\n",
    "      \n",
    "            max_diff = max(max_diff, abs(val - V[s]))\n",
    "\n",
    "            V[s] = val  # Update value with highest value\n",
    "     \n",
    "        if max_diff < delta:\n",
    "            break\n",
    "\n",
    "\n",
    "    for s in states:\n",
    "\n",
    "        val_max = V[s] \n",
    "        for a in actions:\n",
    "            val = rewards[s]  \n",
    "            for s_next in states:\n",
    "                val += probs[s][s_next][a] * (\n",
    "                    gamma * V[s_next]\n",
    "                )  \n",
    "\n",
    "            # Update policy if (i) action improves value and (ii) action different from current policy\n",
    "            if val > val_max and pi[s] != a:\n",
    "                pi[s] = a\n",
    "                val_max = val\n",
    "                optimal_policy_found = False\n",
    "\n",
    "    # If policy did not change, algorithm terminates\n",
    "    if optimal_policy_found:\n",
    "        break\n",
    "\n",
    "    print(V,val_max,V_new,pi,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration (using iterative policy evaluation) for estimating π = π*\n",
    "\n",
    "# 1. Initialization\n",
    "# V (s) 2 R and π(s) = A(s) arbitrarily for all s = S\n",
    "\n",
    "# 2. Policy Evaluation\n",
    "# Loop:\n",
    "# ∆ 0 <- 0\n",
    "# Loop for each s = S*:\n",
    "    # v <- V (s)\n",
    "    # V (s) sum {P a π(a|s) P s 0 ,r p(s 0 , r|s, a) ⇥ r + γV (s 0 )}\n",
    "    # ∆ tends to max(∆, |v V (s)|)\n",
    "# while ∆ < 0\n",
    "\n",
    "# 3. Policy Improvement\n",
    "# policy-stable true\n",
    "# For each s =  S:\n",
    "  # old-action <- π(s)\n",
    "  # π(s) <- sum {P a π(a|s) P s 0 ,r p(s 0 , r|s, a) ⇥ r + γV (s 0 )}\n",
    "  #If old-action 6= π(s), then policy-stable f alse\n",
    "# If policy-stable, then stop and return V ⇡ v⇤ and π ⇡ π⇤; else go to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input π, the policy to be evaluated\n",
    "# Algorithm parameter: a small threshold θ > 0 determining accuracy of estimation\n",
    "# Initialize V (s), for all s = S*, arbitrarily except that V (terminal)=0\n",
    "\n",
    "# 2. Policy Evaluation\n",
    "# Loop:\n",
    "# ∆ 0 <- 0\n",
    "# Loop for each s = S*:\n",
    "    # v <- V (s)\n",
    "    # V (s) sum {P a π(a|s) P s 0 ,r p(s 0 , r|s, a) ⇥ r + γV (s 0 )}\n",
    "    # ∆ tends to max(∆, |v V (s)|)\n",
    "# while ∆ < 0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
