{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0] [0, 0, 0, 0]\n",
      "[70.02109406689799, 17.168692837174138, 51.86037008351223, 10.0] 10.0 [0, 0, 0, 0] [0, 1, 2, 0] 3\n",
      "wtf\n",
      "wtf\n",
      "wtf\n",
      "wtf\n",
      "wtf\n"
     ]
    }
   ],
   "source": [
    "actions = (0,1,2,3)  # actions (0=up, 1=right,2=down,3=left)\n",
    "states = (0, 1, 2, 3)  # states (tiles)\n",
    "rewards = [-1, -1, -1, 10]  # Direct rewards per state\n",
    "gamma = 0.9  \n",
    "delta = 10**-2\n",
    "# Transition probabilities per state-action pair\n",
    "probs = [\n",
    "    [[0.9,0.1,0,0], [0.1,0.8,0.1,0], [0.1,0.1,0.8,0], [0.9,0,0.1,0]],\n",
    "    [[0.1,0.9,0,0], [0,0.9,0,0.1]  , [0.1,0.1,0,0.8], [0.8,0.1,0,0.1]],\n",
    "    [[0.8,0,0.1,0.1], [0.1,0,0.1,0.8] , [0,0,0.9,0.1], [0.1,0,0.9,0]],\n",
    "    [[0,0,0,0]    , [0,0,0,0]    , [0,0,0,0]    , [0,0,0,0]] # Term state (all probs 0)\n",
    "]\n",
    "\n",
    "\n",
    "max_policy_iter = 10000  # Maximum number of policy iterations\n",
    "max_value_iter = 10000  # Maximum number of value iterations\n",
    "pi = [0 for s in states]\n",
    "V = [0 for s in states]\n",
    "\n",
    "print(V,pi)\n",
    "for i in range(max_policy_iter):\n",
    "\n",
    "    optimal_policy_found = True\n",
    "\n",
    "    # Policy evaluation\n",
    "    # Compute value for each state under current policy\n",
    "    for j in range(max_value_iter):\n",
    "        max_diff = 0  # Initialize max difference\n",
    "        V_new = [0, 0, 0, 0]  # Initialize values\n",
    "        for s in states:\n",
    "\n",
    "            # Compute state value\n",
    "            val = rewards[s]  # Get direct reward\n",
    "            for s_next in states:\n",
    "                val += probs[s][s_next][pi[s]] * (\n",
    "                        gamma * V[s_next]\n",
    "                )  \n",
    "            # if np.isnan(np.min(val)):\n",
    "                # print(\"wtf\")\n",
    "      \n",
    "            max_diff = max(max_diff, abs(val - V[s]))\n",
    "\n",
    "            V[s] = val  # Update value with highest value\n",
    "     \n",
    "        if max_diff < delta:\n",
    "            break\n",
    "\n",
    "\n",
    "    for s in states:\n",
    "\n",
    "        val_max = V[s] \n",
    "        for a in actions:\n",
    "            val = rewards[s]  \n",
    "            for s_next in states:\n",
    "                val += probs[s][s_next][a] * (\n",
    "                    gamma * V[s_next]\n",
    "                )  \n",
    "\n",
    "            # Update policy if (i) action improves value and (ii) action different from current policy\n",
    "            if val > val_max and pi[s] != a:\n",
    "                pi[s] = a\n",
    "                val_max = val\n",
    "                optimal_policy_found = False\n",
    "\n",
    "    # If policy did not change, algorithm terminates\n",
    "    if optimal_policy_found:\n",
    "        break\n",
    "\n",
    "    print(V,val_max,V_new,pi,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Value Iteration-------------------------\n",
      "\n",
      "\n",
      "Iteration:  1\n",
      "[[-0.04  -0.04  -0.04   1.592  0.   ]\n",
      " [-0.04  -0.04  -0.04   0.496  0.   ]\n",
      " [-0.04     nan  0.792  0.     0.496]\n",
      " [-0.04  -0.04  -0.04   0.792 -0.04 ]]\n",
      "\n",
      "\n",
      "Iteration:  2\n",
      "[[-0.074   -0.074    1.03576  1.76948  0.     ]\n",
      " [-0.074   -0.074    0.53732  0.74316  0.     ]\n",
      " [-0.074        nan  0.7852   0.       0.4926 ]\n",
      " [-0.074   -0.074    0.56248  0.7852   0.53732]]\n",
      "\n",
      "\n",
      "Iteration:  3\n",
      "[[-0.1029     0.6517368  1.2969582  1.8055744  0.       ]\n",
      " [-0.1029     0.3127976  0.7731576  0.9129186  0.       ]\n",
      " [-0.1029           nan  0.885483   0.         0.5416722]\n",
      " [-0.1029     0.3299064  0.6084888  0.885483   0.5814792]]\n",
      "\n",
      "\n",
      "Iteration:  4\n",
      "[[ 0.38568802  0.95272683  1.36375044  1.82307191  0.        ]\n",
      " [-0.127465    0.56773259  0.98524805  0.95750899  0.        ]\n",
      " [-0.127465           nan  0.90943994  0.          0.54542573]\n",
      " [ 0.16684335  0.42985647  0.68911604  0.89314728  0.65759631]]\n",
      "\n",
      "\n",
      "Iteration:  5\n",
      "[[0.6298032  1.04931386 1.39935377 1.82834938 0.        ]\n",
      " [0.20059881 0.75920773 1.05248464 0.98743498 0.        ]\n",
      " [0.05178443        nan 0.93432095 0.         0.55752668]\n",
      " [0.25564956 0.50167451 0.70321741 0.90647055 0.66959702]]\n",
      "\n",
      "\n",
      "Iteration:  6\n",
      "[[0.74411759 1.08994392 1.41168384 1.83134167 0.        ]\n",
      " [0.42236797 0.82941389 1.08495373 0.99673877 0.        ]\n",
      " [0.14264505        nan 0.94123467 0.         0.56671574]\n",
      " [0.32727056 0.5234725  0.71559073 0.90868923 0.68070549]]\n",
      "\n",
      "\n",
      "Iteration:  7\n",
      "[[0.80031314 1.10523548 1.41752653 1.83238684 0.        ]\n",
      " [0.53780252 0.86091395 1.09688887 1.0015334  0.        ]\n",
      " [0.27145988        nan 0.94504628 0.         0.57505057]\n",
      " [0.35590413 0.53559203 0.72177333 0.91068518 0.68393948]]\n",
      "\n",
      "\n",
      "Iteration:  8\n",
      "[[0.82529996 1.11180807 1.41974836 1.83288322 0.        ]\n",
      " [0.59563936 0.87300714 1.10228393 1.0032586  0.        ]\n",
      " [0.37185389        nan 0.94658629 0.         0.57795814]\n",
      " [0.37752852 0.54185651 0.72556503 0.91148559 0.68628008]]\n",
      "\n",
      "\n",
      "Iteration:  9\n",
      "[[0.83680933 1.11443626 1.42073333 1.83307206 0.        ]\n",
      " [0.62246266 0.87826237 1.1044     1.00405472 0.        ]\n",
      " [0.42824993        nan 0.94736716 0.         0.57979689]\n",
      " [0.39215993 0.54549983 0.72721275 0.91200683 0.68727045]]\n",
      "\n",
      "\n",
      "Iteration:  10\n",
      "[[0.84185477 1.11555283 1.42112533 1.83315578 0.        ]\n",
      " [0.634849   0.88037138 1.10531732 1.004363   0.        ]\n",
      " [0.4560771         nan 0.94768708 0.         0.58062664]\n",
      " [0.40067472 0.54723964 0.72809774 0.91223107 0.68786537]]\n",
      "\n",
      "\n",
      "Iteration:  11\n",
      "[[0.84409575 1.11600921 1.42129355 1.8331891  0.        ]\n",
      " [0.64038558 0.88126934 1.10568805 1.0044979  0.        ]\n",
      " [0.46923042        nan 0.94784028 0.         0.58110172]\n",
      " [0.40494686 0.5481372  0.72848223 0.91235686 0.68813895]]\n",
      "\n",
      "\n",
      "Iteration:  12\n",
      "[[0.84506717 1.11620118 1.42136202 1.83320339 0.        ]\n",
      " [0.64285065 0.88163655 1.10584542 1.00455207 0.        ]\n",
      " [0.47523136        nan 0.94790447 0.         0.58132813]\n",
      " [0.40703837 0.54855124 0.72867339 0.9124128  0.68828812]]\n",
      "\n",
      "\n",
      "Iteration:  13\n",
      "[[0.84548982 1.11628038 1.42139094 1.83320921 0.        ]\n",
      " [0.64393029 0.88179109 1.10590996 1.00457517 0.        ]\n",
      " [0.47792778        nan 0.9479341  0.         0.58144882]\n",
      " [0.40800777 0.54875161 0.72875699 0.91244173 0.68835809]]\n",
      "\n",
      "\n",
      "Iteration:  14\n",
      "[[0.84567136 1.1163135  1.42140284 1.83321167 0.        ]\n",
      " [0.64440123 0.88185485 1.10593708 1.00458461 0.        ]\n",
      " [0.47912032        nan 0.94794669 0.         0.58150665]\n",
      " [0.40845562 0.54884252 0.72879662 0.91245478 0.68839396]]\n",
      "\n",
      "\n",
      "Iteration:  15\n",
      "[[0.84574935 1.11632723 1.42140783 1.83321268 0.        ]\n",
      " [0.64460474 0.88188152 1.10594828 1.00458859 0.        ]\n",
      " [0.47964329        nan 0.94795236 0.         0.58153596]\n",
      " [0.40865687 0.54888493 0.72881402 0.9124612  0.6884108 ]]\n",
      "\n",
      "\n",
      "Iteration:  16\n",
      "[[0.84578261 1.11633295 1.42140989 1.83321311 0.        ]\n",
      " [0.64469236 0.88189257 1.10595296 1.00459023 0.        ]\n",
      " [0.47987058        nan 0.9479548  0.         0.5815499 ]\n",
      " [0.40874727 0.54890397 0.72882203 0.91246411 0.68841909]]\n",
      "\n",
      "\n",
      "Iteration:  17\n",
      "[[0.84579678 1.11633533 1.42141076 1.83321328 0.        ]\n",
      " [0.64472988 0.88189718 1.1059549  1.00459092 0.        ]\n",
      " [0.47996881        nan 0.94795587 0.         0.58155672]\n",
      " [0.40878722 0.54891265 0.72882555 0.9124655  0.68842296]]\n",
      "\n",
      "\n",
      "Iteration:  18\n",
      "[[0.84580279 1.11633632 1.42141111 1.83321336 0.        ]\n",
      " [0.64474589 0.88189909 1.10595571 1.0045912  0.        ]\n",
      " [0.48001101        nan 0.94795634 0.         0.58155993]\n",
      " [0.40880487 0.54891652 0.72882714 0.91246612 0.68842481]]\n",
      "\n",
      "\n",
      "Iteration:  19\n",
      "[[0.84580534 1.11633673 1.42141126 1.83321339 0.        ]\n",
      " [0.6447527  0.88189989 1.10595604 1.00459132 0.        ]\n",
      " [0.48002908        nan 0.94795654 0.         0.58156146]\n",
      " [0.40881259 0.54891826 0.72882783 0.91246642 0.68842567]]\n",
      "\n",
      "\n",
      "Iteration:  20\n",
      "[[0.84580641 1.1163369  1.42141132 1.8332134  0.        ]\n",
      " [0.64475559 0.88190022 1.10595618 1.00459137 0.        ]\n",
      " [0.48003678        nan 0.94795663 0.         0.58156218]\n",
      " [0.40881596 0.54891903 0.72882815 0.91246655 0.68842607]]\n",
      "\n",
      "\n",
      "Iteration:  21\n",
      "[[0.84580686 1.11633697 1.42141135 1.83321341 0.        ]\n",
      " [0.64475681 0.88190036 1.10595624 1.00459139 0.        ]\n",
      " [0.48004005        nan 0.94795667 0.         0.58156251]\n",
      " [0.40881742 0.54891937 0.72882828 0.91246661 0.68842625]]\n",
      "\n",
      "\n",
      "Iteration:  22\n",
      "[[0.84580705 1.116337   1.42141136 1.83321341 0.        ]\n",
      " [0.64475732 0.88190042 1.10595627 1.0045914  0.        ]\n",
      " [0.48004144        nan 0.94795668 0.         0.58156267]\n",
      " [0.40881806 0.54891953 0.72882834 0.91246664 0.68842634]]\n",
      "\n",
      "\n",
      "Iteration:  23\n",
      "[[0.84580713 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.64475754 0.88190044 1.10595628 1.0045914  0.        ]\n",
      " [0.48004203        nan 0.94795669 0.         0.58156274]\n",
      " [0.40881833 0.54891959 0.72882837 0.91246665 0.68842638]]\n",
      "\n",
      "\n",
      "Iteration:  24\n",
      "[[0.84580717 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.64475763 0.88190045 1.10595628 1.0045914  0.        ]\n",
      " [0.48004227        nan 0.94795669 0.         0.58156277]\n",
      " [0.40881845 0.54891962 0.72882838 0.91246665 0.6884264 ]]\n",
      "\n",
      "\n",
      "Iteration:  25\n",
      "[[0.84580718 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.64475767 0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004238        nan 0.9479567  0.         0.58156278]\n",
      " [0.4088185  0.54891963 0.72882839 0.91246666 0.6884264 ]]\n",
      "\n",
      "\n",
      "Iteration:  26\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.64475769 0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004242        nan 0.9479567  0.         0.58156279]\n",
      " [0.40881853 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  27\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004244        nan 0.9479567  0.         0.58156279]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  28\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  29\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  30\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  31\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  32\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  33\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  34\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  35\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  36\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  37\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  38\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  39\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  40\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  41\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  42\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  43\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  44\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  45\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  46\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  47\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  48\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Iteration:  49\n",
      "[[0.84580719 1.11633702 1.42141137 1.83321341 0.        ]\n",
      " [0.6447577  0.88190046 1.10595628 1.0045914  0.        ]\n",
      " [0.48004245        nan 0.9479567  0.         0.5815628 ]\n",
      " [0.40881854 0.54891964 0.72882839 0.91246666 0.68842641]]\n",
      "\n",
      "\n",
      "Final Policy after Convergence: \n",
      "[['R' 'R' 'R' 'R' 'T']\n",
      " ['U' '-' 'U' 'U' 'T']\n",
      " ['U' '-' 'R' 'T' 'D']\n",
      " ['R' 'R' 'U' 'U' 'L']]\n",
      "------------------------Policy Iteration-------------------------\n",
      "\n",
      " Initial Random Policy: \n",
      "[['U' 'L' 'D' 'D' 'D']\n",
      " ['L' 'D' 'R' 'L' 'R']\n",
      " ['D' 'L' 'D' 'U' 'D']\n",
      " ['R' 'U' 'R' 'L' 'L']]\n",
      "\n",
      " Final Policy: \n",
      "[['R' 'R' 'R' 'R' 'T']\n",
      " ['U' '-' 'U' 'U' 'T']\n",
      " ['U' '-' 'R' 'T' 'D']\n",
      " ['R' 'R' 'U' 'U' 'L']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#Extracts data from the Input txt File, assigns it to variables and returns the data to the main() function.\n",
    "def read_inputfile():\n",
    "    walls = []\n",
    "    terminal_states = []\n",
    "    terminal_state_reward = []\n",
    "    transition_probabilities = []\n",
    "    for files in os.listdir():\n",
    "        filename, extension = os.path.splitext(files)\n",
    "        if extension == '.txt':\n",
    "            input_file = files\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            split_line = line.split(\":\")\n",
    "            if split_line[0].strip() == 'size':\n",
    "                interim_size = split_line[1].strip()\n",
    "                size = interim_size.split(\" \")\n",
    "                size = [int(size[0]), int(size[1])]\n",
    "            if split_line[0].strip() == 'walls':\n",
    "                interim_walls = split_line[1].strip()\n",
    "                interim_walls = interim_walls.split(\",\")\n",
    "                for interim_wall in interim_walls:\n",
    "                    interim_wall = interim_wall.strip()\n",
    "                    interim_wall = interim_wall.split(\" \")\n",
    "                    walls.append([int(interim_wall[0]), int(interim_wall[1])])\n",
    "            if split_line[0].strip() == 'terminal_states':\n",
    "                interim_terminalstates = split_line[1].strip()\n",
    "                interim_terminalstates = interim_terminalstates.split(\",\")\n",
    "                for interim_terminalstate in interim_terminalstates:\n",
    "                    interim_terminalstate = interim_terminalstate.strip()\n",
    "                    interim_terminalstate = interim_terminalstate.split(\" \")\n",
    "                    terminal_states.append([int(interim_terminalstate[0]), int(interim_terminalstate[1])])\n",
    "                    terminal_state_reward.append(int(interim_terminalstate[2]))\n",
    "            if split_line[0].strip() == 'reward':\n",
    "                interim_reward = split_line[1].strip()\n",
    "                reward = float(interim_reward)\n",
    "            if split_line[0].strip() == 'transition_probabilities':\n",
    "                interim_TP = split_line[1].strip()\n",
    "                interim_TP = interim_TP.split(\" \")\n",
    "                for interim_tp in interim_TP:\n",
    "                    interim_tp = interim_tp.strip()\n",
    "                    transition_probabilities.append(float(interim_tp))\n",
    "            if split_line[0].strip() == 'discount_rate':\n",
    "                interim_discountrate = split_line[1].strip()\n",
    "                discount_rate = float(interim_discountrate)\n",
    "            if split_line[0].strip() == 'epsilon':\n",
    "                interim_epsilon = split_line[1].strip()\n",
    "                epsilon = float(interim_epsilon)\n",
    "    return size, walls, terminal_states, terminal_state_reward, reward, transition_probabilities, discount_rate, epsilon\n",
    "\n",
    "#Used to create the Initial State based on the mentioned Size of the Matrix, and the position of walls.\n",
    "def create_initialstate(size, walls):\n",
    "    num_rows, num_columns = size\n",
    "    initial_state = np.zeros((num_columns, num_rows))\n",
    "    for wall in walls:\n",
    "        wall_row, wall_column = wall\n",
    "        initial_state[wall_column-1][wall_row-1] = np.nan\n",
    "    return initial_state\n",
    "\n",
    "#Returns a Reward Matrix created based on the mentioned Matrix Size, Position of Terminal States, Correspnding Rewards and Transition Rewards\n",
    "def create_rewardmatrix(size, transition_reward, terminal_states, terminal_state_reward):\n",
    "    num_rows, num_columns = size\n",
    "    reward = np.full((num_columns, num_rows), transition_reward)\n",
    "    for value in range(len(terminal_states)):\n",
    "        state_rows, state_columns = terminal_states[value]\n",
    "        reward[abs(state_columns - num_columns)][abs(state_rows - 1)] = terminal_state_reward[value]\n",
    "    return reward\n",
    "\n",
    "#Returns the List of Possible Actions for Each Intended Action (taking into consideration the stochasticity)\n",
    "def action_predictor(row, column, intended_action, transition_probabilities):\n",
    "    intended_action = str(intended_action[0])\n",
    "    TP = deepcopy(transition_probabilities)\n",
    "    actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    if intended_action == \"U\":\n",
    "        possible_actions = [\"U\", \"L\", \"R\"]\n",
    "    if intended_action == \"D\":\n",
    "        possible_actions = [\"D\", \"R\", \"L\"]\n",
    "    if intended_action == \"L\":\n",
    "        possible_actions = [\"L\", \"U\", \"D\"]\n",
    "    if intended_action == \"R\":\n",
    "        possible_actions = [\"R\", \"U\", \"D\"]\n",
    "    corresponsing_probabilities = [TP[0], TP[1], TP[2]]\n",
    "    return possible_actions, corresponsing_probabilities\n",
    "\n",
    "#With the Position of Current Cell and Action as Inputs, the function returns the Next Cell index(taking into consideration Borders and Wall positions)\n",
    "def action_outcome(initial_state, row, column, action):\n",
    "    max_row, max_column = initial_state.shape\n",
    "    max_row-=1\n",
    "    max_column-=1\n",
    "    if action == \"U\":\n",
    "        if row == 0:\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        elif np.isnan(initial_state[row - 1][column]):\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        else:\n",
    "            next_row = row - 1\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "    if action == \"D\":\n",
    "        if row == max_row:\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        elif np.isnan(initial_state[row + 1][column]):\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        else:\n",
    "            next_row = row + 1\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "    if action == \"L\":\n",
    "        if column == 0:\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        elif np.isnan(initial_state[row][column-1]):\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        else:\n",
    "            next_row = row\n",
    "            next_column = column - 1\n",
    "            return next_row, next_column\n",
    "    if action == \"R\":\n",
    "        if column == max_column:\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        elif np.isnan(initial_state[row][column+1]):\n",
    "            next_row = row\n",
    "            next_column = column\n",
    "            return next_row, next_column\n",
    "        else:\n",
    "            next_row = row\n",
    "            next_column = column + 1\n",
    "            return next_row, next_column\n",
    "\n",
    "#Value Iteration function\n",
    "def value_iteration(initial_state, reward_matrix, transition_probabilities, discount_rate, size, terminal_states, walls):\n",
    "    print(\"-------------------------Value Iteration-------------------------\")\n",
    "    actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    current_state = deepcopy(initial_state)\n",
    "    utility = deepcopy(initial_state)\n",
    "    iterable_utility = deepcopy(initial_state)\n",
    "    row_length, column_length = current_state.shape\n",
    "    previous_utility = np.zeros((row_length, column_length))\n",
    "    final_policy = np.zeros((row_length ,column_length), dtype='U1')\n",
    "    num_rows, num_columns = size\n",
    "    action_cell = []\n",
    "    flag = True\n",
    "    iteration = 0\n",
    "    while flag == True:\n",
    "        #Main Iteration Loop\n",
    "        iteration = iteration + 1\n",
    "        utility = deepcopy(iterable_utility)\n",
    "        for row in range(row_length):\n",
    "            for column in range(column_length):\n",
    "                utility_cell = []\n",
    "                action_tracker = []\n",
    "                actionoutcome_tracker = []\n",
    "                #cell_utility variable\n",
    "                #Cell-wise Operation\n",
    "                for intended_action in actions:\n",
    "                    Q = 0\n",
    "                    action_tracker.append(intended_action)\n",
    "                    possible_actions, corresponsing_probabilities = action_predictor(row, column, intended_action, transition_probabilities)\n",
    "                    for q_value in range(len(possible_actions)):\n",
    "                        next_row, next_column = action_outcome(initial_state, row, column, possible_actions[q_value])\n",
    "                        q_action = corresponsing_probabilities[q_value] * (reward_matrix[next_row][next_column] + (discount_rate * utility[next_row][next_column]))\n",
    "                        Q = Q + q_action\n",
    "                    utility_cell.append(Q)\n",
    "                    actionoutcome_tracker.append(Q)\n",
    "                iterable_utility[row][column] = max(utility_cell)\n",
    "                max_action_index = actionoutcome_tracker.index(max(actionoutcome_tracker))\n",
    "                final_policy[row][column] = action_tracker[max_action_index]\n",
    "        for row in range(row_length):\n",
    "            for column in range(column_length):\n",
    "                if previous_utility[row][column] != iterable_utility[row][column]:\n",
    "                    flag = True\n",
    "                    continue\n",
    "                else:\n",
    "                    flag = False\n",
    "        previous_utility = deepcopy(iterable_utility)\n",
    "        for value in range(len(terminal_states)):\n",
    "            state_rows, state_columns = terminal_states[value]\n",
    "            iterable_utility[abs(state_columns - num_columns)][abs(state_rows - 1)] = 0\n",
    "        print(\"\\n\")\n",
    "        print(\"Iteration: \", iteration)\n",
    "        print(iterable_utility)\n",
    "    for value in range(len(terminal_states)):\n",
    "        state_rows, state_columns = terminal_states[value]\n",
    "        final_policy[abs(state_columns - num_columns)][abs(state_rows - 1)] = 'T'\n",
    "    for wall in walls:\n",
    "        wall_row, wall_column = wall\n",
    "        final_policy[wall_column-1][wall_row-1] = '-'\n",
    "    print(\"\\n\")\n",
    "    print(\"Final Policy after Convergence: \")\n",
    "    print(final_policy)\n",
    "    return iterable_utility, final_policy\n",
    "\n",
    "#Policy Iteration function\n",
    "def policy_iteration(initial_state, reward_matrix, transition_probabilities, discount_rate, size, terminal_states, walls, epsilon):\n",
    "    actions = [\"U\", \"D\", \"L\", \"R\"]\n",
    "    current_state = deepcopy(initial_state)\n",
    "    utility = deepcopy(initial_state)\n",
    "    iterable_utility = deepcopy(initial_state)\n",
    "    row_length, column_length = initial_state.shape\n",
    "    previous_utility = np.zeros((row_length, column_length))\n",
    "    num_rows, num_columns = size\n",
    "    #Initializing Policy Matrix\n",
    "    policy = np.zeros((row_length ,column_length), dtype='U1')\n",
    "    random_action = np.random.randint(0, len(actions), (row_length, column_length))\n",
    "    #Filling Random Actions in Policy to Create Initial Random Policy Matrix\n",
    "    for row in range(row_length):\n",
    "        for column in range(column_length):\n",
    "            policy[row][column] = actions[random_action[row][column]]\n",
    "    initial_random_policy = deepcopy(policy)\n",
    "    outer_flag = True\n",
    "    inner_flag = True\n",
    "    optimal_utility, optimal_policy = value_iteration(initial_state, reward_matrix, transition_probabilities, discount_rate, size, terminal_states, walls)\n",
    "    iteration = 0\n",
    "    while outer_flag is True:\n",
    "        iteration+=1\n",
    "        epsilon_array = []\n",
    "        #Iteration to compare Policy Values\n",
    "        while inner_flag is True:\n",
    "            #Value Iteration\n",
    "            utility = deepcopy(iterable_utility)\n",
    "            for row in range(row_length):\n",
    "                for column in range(column_length):\n",
    "                    intended_action = []\n",
    "                    intended_action.append(policy[row][column])\n",
    "                    utility_cell = []\n",
    "                    for action in intended_action:\n",
    "                        Q = 0\n",
    "                        possible_actions, corresponsing_probabilities = action_predictor(row, column, intended_action, transition_probabilities)\n",
    "                        for q_value in range(len(possible_actions)):\n",
    "                            next_row, next_column = action_outcome(current_state, row, column, possible_actions[q_value])\n",
    "                            q_action = corresponsing_probabilities[q_value] * (reward_matrix[next_row][next_column] + (discount_rate * utility[next_row][next_column]))\n",
    "                            Q = Q + q_action\n",
    "                        utility_cell.append(Q)\n",
    "                    iterable_utility[row][column] = max(utility_cell)\n",
    "            for value in range(len(terminal_states)):\n",
    "                state_rows, state_columns = terminal_states[value]\n",
    "                iterable_utility[abs(state_columns - num_columns)][abs(state_rows - 1)] = 0\n",
    "            for row in range(row_length):\n",
    "                for column in range(column_length):\n",
    "                    if previous_utility[row][column] != iterable_utility[row][column]:\n",
    "                        inner_flag = True\n",
    "                        continue\n",
    "                    else:\n",
    "                        inner_flag = False\n",
    "            previous_utility = deepcopy(iterable_utility)\n",
    "            for value in range(len(terminal_states)):\n",
    "                state_rows, state_columns = terminal_states[value]\n",
    "                iterable_utility[abs(state_columns - num_columns)][abs(state_rows - 1)] = 0\n",
    "\n",
    "        for row in range(row_length):\n",
    "            for column in range(column_length):\n",
    "                value_difference = optimal_utility[row][column] - iterable_utility[row][column]\n",
    "                epsilon_array.append(value_difference)\n",
    "\n",
    "        for row in range(row_length):\n",
    "            for column in range(column_length):\n",
    "                if optimal_utility[row][column] > iterable_utility[row][column]:\n",
    "                    policy[row][column] = optimal_policy[row][column]\n",
    "                else:\n",
    "                    continue\n",
    "        for row in range(row_length):\n",
    "            for column in range(column_length):\n",
    "                if policy[row][column] != optimal_policy[row][column]:\n",
    "                    outer_flag = True\n",
    "                    continue\n",
    "                else:\n",
    "                    outer_flag = False\n",
    "    for value in range(len(terminal_states)):\n",
    "        state_rows, state_columns = terminal_states[value]\n",
    "        policy[abs(state_columns - num_columns)][abs(state_rows - 1)] = 'T'\n",
    "    for wall in walls:\n",
    "        wall_row, wall_column = wall\n",
    "        policy[wall_column-1][wall_row-1] = '-'\n",
    "    print(\"------------------------Policy Iteration-------------------------\")\n",
    "    print(\"\\n Initial Random Policy: \")\n",
    "    print(initial_random_policy)\n",
    "    print(\"\\n Final Policy: \")\n",
    "    print(policy)\n",
    "\n",
    "def main():\n",
    "    #Obtains extracted data from the Input File\n",
    "    size, walls, terminal_states, terminal_state_reward, reward, transition_probabilities, discount_rate, epsilon = read_inputfile()\n",
    "    #Creating Initial State Array\n",
    "    initial_state = create_initialstate(size, walls)\n",
    "    #Creating Reward Matrix Array\n",
    "    reward_matrix = create_rewardmatrix(size, reward, terminal_states, terminal_state_reward)\n",
    "    #Performing Policy Iteration\n",
    "    policy_iteration(initial_state, reward_matrix, transition_probabilities, discount_rate, size, terminal_states, walls, epsilon)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c862ad40c310e1391c7b74cd43a222a44b21eca47ad02380927cc8b64a1d582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('sem6': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
